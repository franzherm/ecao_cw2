{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Particle Swarm Optimisation for Knights Covering Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#necessary python library imports\n",
    "import numpy as np\n",
    "from collections.abc import Callable\n",
    "from tqdm.auto import tqdm\n",
    "import itertools\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Particle Swarm Optimisation class\n",
    "\n",
    "Fitness function is an error function, so we want to minimise it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def none_value_wrapper(value, default):\n",
    "    if value is None:\n",
    "        value = default\n",
    "    return value\n",
    "\n",
    "#features are number of fitness evaluations, fitness value, number of knights, number of covered squares.\n",
    "def store_iteration_results(result_array, index, num_fitness_evals, fitness_value, solution, binary_space: bool = True, board_size = None):\n",
    "    num_covered_squares = 0\n",
    "    if not binary_space:\n",
    "        num_covered_squares = compute_num_of_covered_squares(solution, binary_space= False, board_size=board_size)\n",
    "    else:\n",
    "        num_covered_squares = compute_num_of_covered_squares(solution)\n",
    "    num_knights = np.sum(solution != 0)\n",
    "    \n",
    "    result_array[index,:] = np.array([num_fitness_evals, fitness_value, num_knights, num_covered_squares])\n",
    "\n",
    "def compute_covered_squares(particle_position: np.ndarray, binary_space: bool = True, board_size: int = None) -> np.ndarray:\n",
    "    ''' \n",
    "    Computes all covered squares given the position of a single particle. \n",
    "    Covered squares are defined as positions on the chessboard that are either occupied or attacked by a knight.\n",
    "    '''\n",
    "    if binary_space:\n",
    "        board_size = int(np.sqrt(np.prod(particle_position.shape)))\n",
    "    else:\n",
    "        assert board_size is not None, \"When using integer space, you have to specify the board_size parameter\"\n",
    "        particle_position = convert_integer_to_binary_space(particle_position, board_size)\n",
    "\n",
    "    particle_position = particle_position.reshape((board_size, board_size))\n",
    "    knight_position_indices = np.argwhere(particle_position)\n",
    "\n",
    "    covered_positions = []\n",
    "    relative_jump_positions = [\n",
    "        #(x_change, y_change)\n",
    "        ( 0, 0),#current position\n",
    "        (-1,-2),#left top\n",
    "        (+1,-2),#left bottom\n",
    "        (-1,+2),#right top\n",
    "        (+1,+2),#right bottom\n",
    "        (-2,-1),#top left\n",
    "        (-2,+1),#top right\n",
    "        (+2,-1),#bottom left\n",
    "        (+2,+1),#bottom right\n",
    "    ]\n",
    "\n",
    "    for relative_jump_position in relative_jump_positions:\n",
    "        #calculate positions that the knights can jump to\n",
    "        positions = knight_position_indices.copy()\n",
    "        positions += relative_jump_position   \n",
    "\n",
    "        #calculated positions are only valid if they are within the bounds of the chessboard     \n",
    "        valid_indices = np.all((positions >= 0) & (positions < board_size), axis=1)\n",
    "\n",
    "        #append valid positions to the list of attacked positions\n",
    "        covered_positions.extend(positions[valid_indices])\n",
    "    \n",
    "    covered_positions = np.array(covered_positions)\n",
    "    unique_positions = np.unique(covered_positions, axis=0)\n",
    "\n",
    "    return unique_positions\n",
    "\n",
    "def convert_integer_to_binary_space(particle_position: np.ndarray, board_size: int):\n",
    "    binary_position = np.zeros(shape=board_size**2, dtype=np.uint8) #initialise board\n",
    "    knight_position_indices = np.cumsum(particle_position, dtype=np.uint32) #calculate knight positions on board\n",
    "    knight_position_indices = knight_position_indices - 1 #set the reference point for the first knight to -1 so that 0 always indicates an unused knight\n",
    "    \n",
    "    #keep only valid positions\n",
    "    valid_position_indices = (knight_position_indices >= 0) & (knight_position_indices < board_size**2)\n",
    "    knight_position_indices = knight_position_indices[valid_position_indices]\n",
    "\n",
    "    binary_position[knight_position_indices] = 1 #set knights on board\n",
    "    return binary_position\n",
    "\n",
    "def convert_binary_to_integer_space(particle_position: np.ndarray, max_number_of_knights):\n",
    "    knight_positions = np.argwhere(particle_position == 1).flatten()\n",
    "    num_knights = knight_positions.shape[0]\n",
    "\n",
    "    relative_positions = knight_positions.copy()\n",
    "    relative_positions[1:] -= relative_positions[:-1].copy() #inverse of cumsum as per https://stackoverflow.com/questions/38666924/what-is-the-inverse-of-the-numpy-cumsum-function\n",
    "    binary_positions = np.zeros(max_number_of_knights, dtype=np.uint32)\n",
    "\n",
    "    #store integer positions in return array\n",
    "    if num_knights > max_number_of_knights:\n",
    "        binary_positions[:max_number_of_knights] = relative_positions[:max_number_of_knights]\n",
    "    else:\n",
    "        binary_positions[:num_knights] = relative_positions[:num_knights]\n",
    "\n",
    "    binary_positions[0] = binary_positions[0] + 1 # reference point for first cell is the imaginary cell with index -1\n",
    "    return binary_positions\n",
    "\n",
    "\n",
    "def compute_num_of_covered_squares(particle_position: np.ndarray, binary_space: bool = True, board_size: int = None) -> int:\n",
    "    return compute_covered_squares(particle_position, binary_space, board_size).shape[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Algorithm implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PSO:\n",
    "    def __init__(self, board_size: int, binary_space: bool, fitness_function: Callable[[np.ndarray], np.ndarray], \n",
    "                 penalty_function: Callable[[np.ndarray, int], np.ndarray] = (lambda *x: 0), num_particles: int = 100, c1: float = 1.0, \n",
    "                 c2: float = 1.0, inertia: float = 1.0, max_velocity: float = 4.0, min_velocity: float = -4.0, \n",
    "                 rng: np.random.Generator = np.random.default_rng(None)) -> None:\n",
    "        \n",
    "        #check for sensible hyperparameter selection\n",
    "        assert min_velocity < max_velocity, \"Max velocity has to be greater than the min velocity.\" \n",
    "        assert num_particles > 1, \"There has to be at least one particle\"\n",
    "        assert c1 >= 0, \"c1 has to be greater than 0\"\n",
    "        assert c2 >= 0, \"c2 has to be greater than 0\"\n",
    "        assert board_size > 3, \"board size has to be at least 3\"\n",
    "        #condition on sensible inertia values as proposed in: \"F. van den Bergh. An Analysis of Particle Swarm Optimizers (2002)\"\n",
    "        assert inertia > c1+c2 / 2 - 1 and inertia >= 0, \"Inertia must be positive and greater than (c1 + c2) / 2 -1\"\n",
    "\n",
    "        #assign hyperparameters to object variables\n",
    "        self.board_size = board_size\n",
    "        self.binary_space = binary_space\n",
    "        self.fitness_function = fitness_function\n",
    "        self.penalty_function = none_value_wrapper(penalty_function, default=(lambda *x: 0))\n",
    "        self.num_particles = none_value_wrapper(num_particles, default=100)\n",
    "        self.c1 = none_value_wrapper(c1, default=1.0)\n",
    "        self.c2 = none_value_wrapper(c2, default=1.0)\n",
    "        self.max_velocity = none_value_wrapper(max_velocity,4.0)\n",
    "        self.min_velocity = none_value_wrapper(min_velocity,-4.0)\n",
    "        self.inertia = none_value_wrapper(inertia, default=1.0)\n",
    "        self.rng = none_value_wrapper(rng, np.random.default_rng(None))\n",
    "\n",
    "        #initialise particles and velocities\n",
    "        self.particle_positions = self.initialise_particle_positions(self.num_particles,self.binary_space, self.board_size)\n",
    "        self.particle_best_pos = self.particle_positions.copy()\n",
    "        \n",
    "        #set initial velocity to be in range [min_velocity, max_velocity]\n",
    "        random_velocities_factors = self.rng.random(size=(self.num_particles, self.particle_positions.shape[1]))\n",
    "        self.particle_velocities = random_velocities_factors * (self.max_velocity - self.min_velocity) + self.min_velocity\n",
    "        \n",
    "        #initialise global best\n",
    "        self.particle_fitness = self.compute_penalty_fitness(self.particle_positions, 0, self.binary_space, self.board_size)\n",
    "        self.particle_best_fitness = self.particle_fitness.copy()\n",
    "\n",
    "        best_fitness = np.min(self.particle_fitness) #find best fitness value\n",
    "        best_indices = np.argwhere(self.particle_fitness == best_fitness) #find all particles that have the best fitness value\n",
    "        best_chosen_index = self.rng.choice(best_indices) #randomly choose one of the particles with the best fitness values\n",
    "\n",
    "        self.global_best_location = self.particle_positions[best_chosen_index].copy()\n",
    "        self.global_best_fitness = self.particle_fitness[best_chosen_index][0]\n",
    "    \n",
    "    def initialise_particle_positions(self, num_particles: int, binary_space: bool, board_size: int):\n",
    "        initial_positions = None\n",
    "        if binary_space: #for binary space\n",
    "            initial_positions = self.rng.integers(low=0, high=1, endpoint=True, size=(num_particles, board_size**2))\n",
    "\n",
    "        else: #for integer space\n",
    "            #formular for maximum number of knights as described in: https://mathworld.wolfram.com/KnightsProblem.html\n",
    "            max_num_of_knights = (board_size**2)/2\n",
    "            if board_size % 2 == 1:\n",
    "                max_num_of_knights += 0.5\n",
    "            max_num_of_knights = int(max_num_of_knights)\n",
    "\n",
    "            chess_boards = np.zeros(shape=(num_particles, board_size**2), dtype=np.uint8)\n",
    "            num_knights = self.rng.integers(low=1, high=max_num_of_knights,endpoint=True, size=num_particles)\n",
    "            indices = [self.rng.choice(board_size**2, size=x, replace=False) for x in num_knights]\n",
    "            \n",
    "            for i in range(len(indices)):\n",
    "                chess_boards[i,indices[i]] = 1\n",
    "\n",
    "            initial_positions = np.apply_along_axis(convert_binary_to_integer_space, axis=1,arr=chess_boards,max_number_of_knights=max_num_of_knights)\n",
    "\n",
    "\n",
    "        return initial_positions\n",
    "\n",
    "    def compute_penalty_fitness(self, particle_positions: np.ndarray, k: int, binary_space: bool, board_size:int):\n",
    "        ''' Computes the fitness subtracted by the penalty for invalid solutions (i.e. not all squares attacked or occupied).\n",
    "            The penalty function receives a parameter 'k', which indicates the current iteration number.\n",
    "        '''\n",
    "        if binary_space:\n",
    "            return self.fitness_function(particle_positions) + self.penalty_function(particle_positions, k)\n",
    "        else:\n",
    "            return self.fitness_function(particle_positions, board_size) + self.penalty_function(particle_positions, k, board_size)\n",
    "    \n",
    "    def update_locations(self, r_sigmoid, binary_space):\n",
    "        if binary_space:\n",
    "            #the original algorithm was flawed in regards to the position update\n",
    "            #see https://www.researchgate.net/publication/224302958_A_novel_binary_particle_swarm_optimization\n",
    "            sigmoid = 1/(1 + np.exp(-self.particle_velocities))\n",
    "            change_indices = r_sigmoid < sigmoid\n",
    "\n",
    "            #dimensions in which the normalised velocity value is greater than random value change their location value from 0 to 1 or vice versa\n",
    "            self.particle_positions[change_indices] == (self.particle_positions[change_indices] + 1) % 2\n",
    "\n",
    "        else:\n",
    "            #adds velocity to current position. Rounds to nearest integer and clips so that the numbers are always non-negative.\n",
    "            new_positions = np.clip(np.rint(self.particle_positions + self.particle_velocities),a_min=0, a_max=self.board_size**2)\n",
    "\n",
    "            new_positions_array = np.zeros(shape=self.particle_positions.shape)\n",
    "            non_zero_indices = new_positions != 0\n",
    "\n",
    "            #shifts the zeros to the end of the rows while maintaining the relative order of all the other elements in the row\n",
    "            for i, row in enumerate(new_positions):\n",
    "                non_zero_elements = row[non_zero_indices[i]]\n",
    "                new_positions_array[i,:len(non_zero_elements)] = non_zero_elements\n",
    "            \n",
    "            #update particle positions array\n",
    "            self.particle_positions = new_positions_array\n",
    "\n",
    "    def optimise(self, num_fitness_evaluations):\n",
    "        ''' Starts the optimisation loop. The num_iterations parameter specifies the terimination criterion as \n",
    "            the number of iterations that will be done.\n",
    "        '''\n",
    "        num_iterations = num_fitness_evaluations // self.num_particles\n",
    "\n",
    "        #features are number of fitness evaluations, fitness value, number of knights, number of covered squares.\n",
    "        #Only current best solutions are stored\n",
    "        metric_results = np.full(shape=(num_iterations, 4), fill_value=-1)\n",
    "        stored_solutions = []\n",
    "\n",
    "        store_iteration_results(metric_results, index=0, num_fitness_evals=0, fitness_value=self.global_best_fitness,\n",
    "                                solution=self.global_best_location, binary_space=self.binary_space, board_size=self.board_size)\n",
    "        stored_solutions.append(self.global_best_location if self.binary_space \n",
    "                                else convert_integer_to_binary_space(self.global_best_location,self.board_size))\n",
    "\n",
    "\n",
    "        # initialise random numbers used in velocity computation\n",
    "        r1 = np.empty(shape=(self.num_particles,1))\n",
    "        r2 = np.empty(shape=(self.num_particles,1))\n",
    "        r_sigmoid = np.empty(shape=(self.num_particles,1))\n",
    "\n",
    "        for i in range(1, num_iterations):\n",
    "            #update random numbers\n",
    "            self.rng.random(size=(self.num_particles,1), out=r1)\n",
    "            self.rng.random(size=(self.num_particles,1), out=r2)\n",
    "            self.rng.random(size=(self.num_particles,1), out=r_sigmoid)\n",
    "\n",
    "            # update velocities\n",
    "            reduced_current_velocity = self.inertia * self.particle_velocities\n",
    "            local_velocity_component = self.c1 * r1 * (self.particle_best_pos - self.particle_positions)\n",
    "            global_velocity_component = self.c2 * r2 * (self.global_best_location - self.particle_positions)\n",
    "\n",
    "            new_velocity = reduced_current_velocity + local_velocity_component + global_velocity_component\n",
    "            self.particle_velocities = np.clip(new_velocity, a_max= self.max_velocity, a_min= self.min_velocity)\n",
    "\n",
    "            # update positions of particles in place\n",
    "            self.update_locations(r_sigmoid, binary_space=self.binary_space)\n",
    "\n",
    "            # calculate fitness\n",
    "            self.particle_fitness = self.compute_penalty_fitness(self.particle_positions, i, self.binary_space, self.board_size)\n",
    "\n",
    "            # update best positions\n",
    "            # local best positions\n",
    "            new_best_local_fitness_indices = self.particle_fitness > self.particle_best_fitness\n",
    "            self.particle_best_fitness[new_best_local_fitness_indices] = self.particle_fitness[new_best_local_fitness_indices]\n",
    "            self.particle_best_pos[new_best_local_fitness_indices] = self.particle_positions[new_best_local_fitness_indices]\n",
    "\n",
    "            # global best positions\n",
    "            new_best_global_fitness = np.min(self.particle_fitness) #find best fitness value\n",
    "            new_best_global_fitness_indices = np.argwhere(self.particle_fitness == new_best_global_fitness) #find all particles that have the best fitness value\n",
    "            new_best_chosen_global_fitness_index = self.rng.choice(new_best_global_fitness_indices) #randomly choose one of the particles with the best fitness values\n",
    "\n",
    "            #update best solution \n",
    "            if new_best_global_fitness < self.global_best_fitness:\n",
    "                self.global_best_fitness = new_best_global_fitness\n",
    "                self.global_best_location = self.particle_positions[new_best_chosen_global_fitness_index].copy()\n",
    "            \n",
    "            #store solution and metric values to arrays\n",
    "            store_iteration_results(metric_results, index=i, num_fitness_evals=i*self.num_particles, fitness_value=self.global_best_fitness,\n",
    "                                    solution=self.global_best_location, binary_space=self.binary_space, board_size=self.board_size)\n",
    "            stored_solutions.append(self.global_best_location if self.binary_space else convert_integer_to_binary_space(self.global_best_location, board_size=self.board_size))\n",
    "        \n",
    "        return metric_results, stored_solutions\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitness and penalty functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fitness function implementations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fitness_space_pso_paper(particle_positions: np.ndarray, board_size:int = None) -> np.ndarray:\n",
    "    '''From paper Investigating binary PSO parameter influence on the knights cover problem by N. Franken and A.P. Engelbrecht (2005)'''\n",
    "\n",
    "    if board_size is not None:\n",
    "        return fitness_integer_space_pso_paper(particle_positions, board_size)\n",
    "    else:\n",
    "        num_covered_squares = np.apply_along_axis(compute_num_of_covered_squares, axis=1, arr = particle_positions)\n",
    "        num_knights = np.sum(particle_positions, axis=1)\n",
    "        total_num_squares = particle_positions.shape[1]\n",
    "        num_empty_squares = total_num_squares - num_covered_squares\n",
    "        \n",
    "        eps = 1e-6 #to avoid zero division\n",
    "        fitness = num_empty_squares + num_knights + total_num_squares/(num_covered_squares + eps)\n",
    "\n",
    "        return fitness\n",
    "\n",
    "def fitness_integer_space_pso_paper(particle_positions: np.ndarray, board_size:int = None) -> np.ndarray:\n",
    "    num_covered_squares = np.apply_along_axis(compute_num_of_covered_squares, axis=1, arr=particle_positions, binary_space=False, board_size=board_size)\n",
    "    num_knights = np.sum((particle_positions > 0), axis=1)\n",
    "    total_num_squares = board_size**2\n",
    "    num_empty_squares = total_num_squares - num_covered_squares\n",
    "   \n",
    "    eps = 1e-6 #to avoid zero division\n",
    "    \n",
    "    fitness = num_empty_squares + num_knights + total_num_squares/(num_covered_squares + eps)\n",
    "    return fitness\n",
    "\n",
    "def fitness_binary_space_simple(particle_positions: np.ndarray) -> np.ndarray:\n",
    "    num_knights = np.sum(particle_positions, axis=1)\n",
    "    return num_knights\n",
    "\n",
    "def fitness_integer_space_simple(particle_positions: np.ndarray, board_size:int = None) -> np.ndarray:\n",
    "    num_knights = np.sum((particle_positions > 0), axis=1)\n",
    "    return num_knights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Penalty function implementations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#computes penalty for conventional search space depending on the ratio of empty squares\n",
    "def penalty_binary_space(particle_positions: np.ndarray, iteration_number: int):\n",
    "    ratio_of_empty_squares = (particle_positions.shape[1] - compute_num_of_covered_squares(particle_positions)) / particle_positions.shape[1]\n",
    "\n",
    "    penalties = iteration_number * ratio_of_empty_squares #use ratio so that the penalty is independent of the board size\n",
    "    return penalties\n",
    "\n",
    "#computes penalty for integer search space depending on ratio of empty squares and whether the knights are positioned outside the board\n",
    "def penalty_integer_space(particle_positions:np.ndarray, iteration_number: int, board_size):\n",
    "    ratio_of_empty_squares = (board_size**2 - compute_num_of_covered_squares(particle_positions)) / board_size**2\n",
    "    last_knight_location = np.sum(particle_positions, axis=1)\n",
    "    distance_from_last_square = last_knight_location - board_size**2\n",
    "\n",
    "    penalties = iteration_number * (ratio_of_empty_squares + np.maximum(0, distance_from_last_square))\n",
    "    return penalties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Search Algorithm class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomSearch:\n",
    "    def __init__(self, board_size: int, fitness_function: Callable[[np.ndarray], np.ndarray], \n",
    "                 penalty_function: Callable[[np.ndarray, int], np.ndarray] = (lambda *x: 0), \n",
    "                 rng: np.random.Generator = np.random.default_rng(None)):\n",
    "        \n",
    "        self.board_size = board_size\n",
    "        self.fitness_function = fitness_function\n",
    "        self.penalty_function = none_value_wrapper(penalty_function, default=(lambda *x: 0))\n",
    "        self.rng = rng\n",
    "\n",
    "        self.current_solution = self.rng.integers(low=0, high=1, endpoint=True, size=board_size**2)\n",
    "        #add another dimension so that fitness functions work the same across all algorithms\n",
    "        self.current_solution = self.current_solution[np.newaxis,:]\n",
    "        self.best_solution = self.current_solution.copy()\n",
    "        self.best_fitness = self.compute_penalty_fitness(self.best_solution, 0)[0]\n",
    "        \n",
    "    def compute_penalty_fitness(self, particle_positions: np.ndarray, k: int):\n",
    "        ''' Computes the fitness subtracted by the penalty for invalid solutions (i.e. not all squares attacked or occupied).\n",
    "            The penalty function receives a parameter 'k', which indicates the current iteration number.\n",
    "        '''\n",
    "        return self.fitness_function(particle_positions) + self.penalty_function(particle_positions, k)\n",
    "        \n",
    "    def optimise(self, num_iterations):\n",
    "        \n",
    "        #features are number of fitness evaluations, fitness value, number of knights, number of covered squares.\n",
    "        #Only current best solutions are stored\n",
    "        metric_results = np.full(shape=(num_iterations, 4), fill_value=-1)\n",
    "        stored_solutions = []\n",
    "\n",
    "        store_iteration_results(metric_results, index=0, num_fitness_evals=0, fitness_value=self.best_fitness,solution=self.best_solution)\n",
    "        stored_solutions.append(self.best_solution)\n",
    "\n",
    "        for i in range(1, num_iterations):\n",
    "            #select random position to be altered\n",
    "            mutation_index = self.rng.integers(low=0, high=self.current_solution.shape[0])\n",
    "            #set element at index position to it's binary complement\n",
    "            self.current_solution[mutation_index] = (self.current_solution[mutation_index]+1) % 2 \n",
    "            #compute fitness of altered solution\n",
    "            fitness = self.compute_penalty_fitness(self.current_solution, i)[0]\n",
    "            \n",
    "            #update best solution\n",
    "            if(fitness < self.best_fitness):\n",
    "                self.best_solution = self.current_solution.copy()\n",
    "                self.best_fitness = fitness\n",
    "            \n",
    "            store_iteration_results(metric_results, index=i, num_fitness_evals=i, fitness_value=self.best_fitness, solution=self.best_solution)\n",
    "            stored_solutions.append(self.best_solution)\n",
    "        \n",
    "        return metric_results, stored_solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic Hillclimb Algorithm class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#implements First choice stochastic hill climb as per Artificial Intelligence: A Modern Approach, Global Edition, Fourth Edition\n",
    "class StochasticHillclimb:\n",
    "    def __init__(self, board_size: int, fitness_function: Callable[[np.ndarray], np.ndarray], \n",
    "                 penalty_function: Callable[[np.ndarray, int], np.ndarray] = (lambda *x: 0), allowed_plateau_steps: int = 100,\n",
    "                 plateau_tolerance: float = 0.1, rng: np.random.Generator = np.random.default_rng(None)):\n",
    "        \n",
    "        assert plateau_tolerance >= 0\n",
    "\n",
    "        self.board_size = board_size\n",
    "        self.fitness_function = fitness_function\n",
    "        self.penalty_function = none_value_wrapper(penalty_function, default=(lambda *x: 0))\n",
    "        self.rng = rng\n",
    "        self.allowed_plateau_steps = allowed_plateau_steps\n",
    "        self.plateau_tolerance = plateau_tolerance\n",
    "        self.current_solution = self.rng.integers(low=0, high=1, endpoint=True, size=board_size**2)\n",
    "\n",
    "        #add another dimension so that fitness functions work the same across all algorithms\n",
    "        self.current_fitness = self.compute_penalty_fitness(self.current_solution[np.newaxis,:], 0)[0]\n",
    "\n",
    "    def compute_penalty_fitness(self, particle_positions: np.ndarray, k: int):\n",
    "        ''' Computes the fitness subtracted by the penalty for invalid solutions (i.e. not all squares attacked or occupied).\n",
    "            The penalty function receives a parameter 'k', which indicates the current iteration number.\n",
    "        '''\n",
    "        return self.fitness_function(particle_positions) + self.penalty_function(particle_positions, k)\n",
    "\n",
    "    def optimise(self, num_iterations):\n",
    "        num_plateau_steps = 0\n",
    "\n",
    "        #features are number of fitness evaluations, fitness value, number of knights, number of covered squares.\n",
    "        #Only current best solutions are stored\n",
    "        metric_results = np.full(shape=(num_iterations, 4), fill_value=-1)\n",
    "        stored_solutions = []\n",
    "\n",
    "        store_iteration_results(metric_results, index=0, num_fitness_evals=0, fitness_value=self.current_fitness, solution=self.current_solution)\n",
    "        stored_solutions.append(self.current_solution)\n",
    "\n",
    "        for i in range(1, num_iterations):\n",
    "            neighbour_permutation = self.rng.permutation(self.current_solution.shape[0])\n",
    "            \n",
    "            better_solution_found = False\n",
    "            for neighbour_index in neighbour_permutation:\n",
    "                neighbour_solution = self.current_solution.copy()\n",
    "                neighbour_solution[neighbour_index] = (neighbour_solution[neighbour_index]+1) % 2 \n",
    "                \n",
    "                #add another dimension so that fitness functions work the same across all algorithms\n",
    "                neighbour_fitness = self.compute_penalty_fitness(neighbour_solution[np.newaxis,:], i)[0]\n",
    "\n",
    "                #choose first neighbour with more optimal fitness (in this case, lower fitness value as we want to minimise the function)\n",
    "                # neighbour has either significantly better fitness than current solution or is within plateau tolerance\n",
    "                if (neighbour_fitness - self.current_fitness) < self.plateau_tolerance:\n",
    "                    #check if fitness is on a plateau\n",
    "                    if abs(neighbour_fitness - self.current_fitness) < self.plateau_tolerance:\n",
    "                        num_plateau_steps += 1 #increment plateau step counter\n",
    "                        \n",
    "                        #cancel optimisation if maximum number of plateau steps is reached\n",
    "                        if num_plateau_steps > self.allowed_plateau_steps:\n",
    "                            return metric_results, stored_solutions\n",
    "                        \n",
    "                    else: #reset plateau step counter if fitness is reduced\n",
    "                        num_plateau_steps = 0\n",
    "\n",
    "                    self.current_solution = neighbour_solution\n",
    "                    self.current_fitness = neighbour_fitness\n",
    "\n",
    "                    store_iteration_results(metric_results, index=i, num_fitness_evals=i, fitness_value=self.current_fitness, solution=self.current_solution)\n",
    "                    stored_solutions.append(self.current_solution)\n",
    "                    better_solution_found = True\n",
    "                    break\n",
    "            \n",
    "            # no improved solution was found among the neighbours -> peak has been reached\n",
    "            if not better_solution_found:\n",
    "                return metric_results, stored_solutions\n",
    "\n",
    "        #after final iteration\n",
    "        return metric_results, stored_solutions \n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def increment_indices(current_indices, max_indices):\n",
    "    current_indices[-1] += 1\n",
    "    for i in reversed(range(len(current_indices))):\n",
    "        if current_indices[i] > max_indices[i]:\n",
    "            current_indices[i] = 0\n",
    "            if i != 0:\n",
    "                current_indices[i-1] +=1\n",
    "        else:\n",
    "            break\n",
    "    return current_indices\n",
    "\n",
    "def fetch_algorithm_parameters(algorithm_config, parameter_indices):\n",
    "    config_value_list = list(algorithm_config.values())\n",
    "    parameters = [config_value_list[parameter][parameter_index] for parameter, parameter_index in enumerate(parameter_indices)]\n",
    "    \n",
    "    #flatten tuples in parameter list\n",
    "    flattened_parameters = []\n",
    "    for element in parameters:\n",
    "        if isinstance(element,tuple):\n",
    "            flattened_parameters.extend(element)\n",
    "        else:\n",
    "            flattened_parameters.append(element)\n",
    "            \n",
    "    return flattened_parameters\n",
    "\n",
    "def create_dataframe_from_results(algorithm_config, parameter_values_list, metric_results_list, \n",
    "                                  solutions_list, num_of_experiments, METRIC_FEATURE_NAMES, algorithm_name):\n",
    "    #extract function names from parameter list\n",
    "    for i in range(len(parameter_values_list)):\n",
    "        parameter_values_list[i] = [elem.__name__ if callable(elem) else elem for elem in parameter_values_list[i]]\n",
    "\n",
    "    #calculate the number of rows in metric_results_list that correspond to the same experiment\n",
    "    num_rows_per_experiment = metric_results_list.shape[0] // num_of_experiments\n",
    "\n",
    "    #extend parameter_values_list to have the same number of elements as metric_results_list\n",
    "    parameter_values_list = list(itertools.chain.from_iterable(itertools.repeat(x, num_rows_per_experiment) for x in parameter_values_list))\n",
    "\n",
    "    #extract column header names from algorithm config dict\n",
    "    param_column_headers = list(algorithm_config.keys())\n",
    "    param_column_headers = [str.split(key,\"_and_\") for key in param_column_headers] #split compound keys\n",
    "    param_column_headers = list(itertools.chain.from_iterable(param_column_headers))#flatten list\n",
    "\n",
    "    #convert numpy solution arrays to strings\n",
    "    solutions_list = [np.array2string(x.flatten()) for x in solutions_list]\n",
    "\n",
    "    #create dataframes for metric values, parameter values, solutions and for the metadata valid for all conducted experiments\n",
    "    df_metric_values = pd.DataFrame(metric_results_list, columns= METRIC_FEATURE_NAMES)\n",
    "    df_param_values = pd.DataFrame(parameter_values_list, columns = param_column_headers)\n",
    "    df_solutions = pd.Series(solutions_list, name=\"solutions\")\n",
    "\n",
    "    #concatenate columns of dataframes\n",
    "    df_results = pd.concat([df_param_values,df_metric_values], axis=1)\n",
    "    df_results = df_results.loc[(df_metric_values!=-1).any(axis=1)] #remove unused rows\n",
    "    df_results = df_results.assign(solutions=solutions_list) #append solutions\n",
    "\n",
    "    df_results[\"algorithm\"] = algorithm_name #append information on which search algorithm was used\n",
    "    return df_results\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimentation config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run configuration for particle swarm optimisation\n",
    "pso_config = {\n",
    "    \"binary_space\": [True, False],\n",
    "    \"fitness_function_and_penalty_function\": [(fitness_space_pso_paper,None)],\n",
    "    \"num_particles\": [100],\n",
    "    \"c1_and_c2_and_inertia\": [(1,1,1)],\n",
    "    \"max_velocity_and_min_velocity\": [(4,-4)],\n",
    "}\n",
    "\n",
    "#run configuration for random search\n",
    "rs_config = {\n",
    "    \"fitness_function_and_penalty_function\": [(fitness_space_pso_paper, None)]\n",
    "}\n",
    "\n",
    "#run configuration for stochastic hillclimb\n",
    "shc_config = {\n",
    "    \"fitness_function_and_penalty_function\": [(fitness_space_pso_paper, None)],\n",
    "    \"allowed_plateau_steps\": [100,200],\n",
    "    \"plateau_tolerance\": [0.1],\n",
    "}\n",
    "\n",
    "#meta run config for all experiments\n",
    "run_config = {\n",
    "    \"optimisation_algorithm\": [PSO, RandomSearch, StochasticHillclimb],\n",
    "    \"algorithm_configs\":[pso_config, rs_config, shc_config],\n",
    "    \"board_sizes\": list(np.arange(start=4, stop=6)),\n",
    "    \"runs_per_experiment\": 10,\n",
    "    \"function_evals_per_run\": 1000,\n",
    "    \"rng_seed\": 100\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c67597adf00440409be280c62ce095ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Optimisation Algorithms:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "758ebf8cfe51477c89f60d4469f6ac7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Experiments:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "978db5dee1a04bf5b754918d2ec24bfa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Runs:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d31761107634604affb3dda2db4db95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Runs:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0889448af577427c829608fc419e4390",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Runs:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "faaf937ca5b7446aa665979cfc64c34e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Runs:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e16aff2f45ec426296b63881c717c5f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Experiments:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d860f93bfe284f36b0a966d957a4df45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Runs:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33800622643648259145f3e59c793186",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Runs:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c04b8f9012504adda2722c7a9c19d3f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Experiments:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3ad67bd54934827bf8d9adf2bc18ef2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Runs:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0413bd49a18d49018bfb4cff19732467",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Runs:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78ec02f96ab54ec79fbbe457814658d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Runs:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b4ea2f7e61245b2877723121558f11b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Runs:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#TODO: store results in numpy array and then to pandas dataframe\n",
    "rng = np.random.default_rng(run_config[\"rng_seed\"])\n",
    "NUM_METRIC_FEATURES = 4\n",
    "METRIC_FEATURE_NAMES = [\"fitness_evals\",\"fitness_values\",\"num_knights\",\"num_covered_squares\"]\n",
    "dataframe_list = []\n",
    "# run each optimisation algorithm\n",
    "for optimiser_id, algorithm in tqdm(enumerate(run_config[\"optimisation_algorithm\"]), \n",
    "    desc=\"Optimisation Algorithms\", total=len(run_config[\"optimisation_algorithm\"]), position=0, leave=False):\n",
    "\n",
    "    algorithm_config = run_config[\"algorithm_configs\"][optimiser_id] #fetch experiment config for the current optimisation algorithm\n",
    "    \n",
    "    #add board_sizes from run_config as first element of current config\n",
    "    extended_config = {\"board_sizes\":run_config[\"board_sizes\"]} \n",
    "    extended_config.update(algorithm_config)\n",
    "    algorithm_config = extended_config\n",
    "\n",
    "    max_parameter_indices = np.array([len(x)-1 for x in algorithm_config.values()], dtype=int) #compute the maximum indices for each config parameter\n",
    "    num_of_experiments = np.cumprod(max_parameter_indices + 1)[-1] #compute the number of experiments to run based on number of values in config\n",
    "    current_parameter_indices = np.zeros(len(algorithm_config), dtype= int) #initialise current parameter indices to 0\n",
    "\n",
    "    #initialise result storage. When using PSO, the list will not be completely filled \n",
    "    #because less than function_evals_per_run solutions will be provided per run. These will be removed when storing the results to the dataframe\n",
    "    metric_results_list = np.full(shape=(run_config[\"runs_per_experiment\"]*num_of_experiments*run_config[\"function_evals_per_run\"],\n",
    "                                   NUM_METRIC_FEATURES), fill_value=-1)\n",
    "    stored_solutions_list = []\n",
    "    parameter_values_list = []\n",
    "    \n",
    "    #run all experiments\n",
    "    for experiment_index in tqdm(range(num_of_experiments), desc=\"Experiments\", position=1, leave=False):\n",
    "        parameters = fetch_algorithm_parameters(algorithm_config, current_parameter_indices)\n",
    "        parameter_values_list.append(parameters.copy())\n",
    "        parameters.append(rng) #append random number generator to parameters\n",
    "\n",
    "        #run all runs of the same experiment\n",
    "        for run in tqdm(range(run_config[\"runs_per_experiment\"]), desc=\"Runs\", position=2, leave=False):\n",
    "            optimiser = algorithm(*parameters) #initialise optimisation algorithm with parameters\n",
    "            metric_results, stored_solutions  = optimiser.optimise(run_config[\"function_evals_per_run\"]) #perform optimisation\n",
    "\n",
    "            start_index = experiment_index*run_config[\"runs_per_experiment\"]*run_config[\"function_evals_per_run\"] \\\n",
    "                          +run*run_config[\"function_evals_per_run\"]\n",
    "            metric_results_list[start_index:start_index+metric_results.shape[0],:] = metric_results\n",
    "            stored_solutions_list.extend(stored_solutions)\n",
    "\n",
    "        #increment indices of the current algorithm's parameters\n",
    "        current_parameter_indices = increment_indices(current_parameter_indices, max_parameter_indices)\n",
    "    \n",
    "    df = create_dataframe_from_results(algorithm_config, parameter_values_list, metric_results_list, stored_solutions_list, num_of_experiments,\n",
    "                                       METRIC_FEATURE_NAMES, algorithm_name= algorithm.__name__)\n",
    "    dataframe_list.append(df)\n",
    "\n",
    "# save dataframes to csv files\n",
    "for i, dataframe in enumerate(dataframe_list):\n",
    "    dataframe.to_csv(run_config[\"optimisation_algorithm\"][i].__name__+\"_results.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store experiment results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result Visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load results file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[True, True, True, [1, 2, 5], [1, 2, 5], [1, 2, 5], 'blubb', 'blubb', 'blubb']\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "lst = [True, [1,2, 5], \"blubb\"]\n",
    "blubb = list(itertools.chain.from_iterable(itertools.repeat(x, 3) for x in lst))\n",
    "print(blubb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
